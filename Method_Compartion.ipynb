{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall performance comparasion: \n",
    "\n",
    "Data: 55000 training data, 5000 validation data, 10000 test data; \n",
    "Step up: 5 layers with 100 node on each layer. \n",
    "\n",
    "Batch GD: waist a lot of time to converge. It needed 2000 step to have similar performance of mini-batch.\n",
    "\n",
    "    time consumption: 1484.6108751296997\n",
    "    performance score: 0.865\n",
    "    \n",
    "mini-batch GD: \n",
    "\n",
    "    time consumption: 89.35848474502563\n",
    "    performance score: 0.9733\n",
    "Adam: \n",
    "    \n",
    "    time consumption:  114.90625596046448\n",
    "    performance score:   0.1998\n",
    "\n",
    "Adam with earily stoping: (35% threshold stop)\n",
    "\n",
    "    time consumption: 51.90262746810913\n",
    "    performance score: 0.9\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 784) (10000, 784)\n",
      "(60000,) (10000,)\n",
      "(5000, 784) (55000, 784) (5000,)\n"
     ]
    }
   ],
   "source": [
    "(X_data, y_data), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print (X_data.shape)\n",
    "# reduce the dimention of X_train, X_test\n",
    "def reduce_D (data):\n",
    "    return data.astype (np.float32).reshape (-1, 28 * 28) / 255.0\n",
    "X_data, X_test = reduce_D  (X_data), reduce_D (X_test)\n",
    "print (X_data.shape, X_test.shape)\n",
    "\n",
    "# Check Y :\n",
    "# print (y_data.shape)\n",
    "# def y_preprocess (data):\n",
    "#     return data.astype (np.int32).reshape (data.shape[0], 1)\n",
    "# y_data, y_test = y_preprocess (y_data), y_preprocess (y_test)\n",
    "print (y_data.shape, y_test.shape)\n",
    "\n",
    "# split the train and validation data:\n",
    "X_validation, y_validation = X_data[:5000], y_data[:5000]\n",
    "X_train, y_train = X_data[5000:], y_data[5000:]\n",
    "\n",
    "print (X_validation.shape, X_train.shape, y_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step up: 5 layers with 100 nodes on each layer, mini-batch size is 32, train 20 steps.\n",
    "\n",
    "Performance evaluation: \n",
    "\n",
    "    time consumption: 89.35848474502563\n",
    "    performance score: 0.9733"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_feature = X_test.shape[1]\n",
    "n_layers = 5\n",
    "n_neurons = 100\n",
    "learning_rate = 0.01\n",
    "n_output = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder (tf.float32, shape = (None, N_feature), name = 'X')\n",
    "y = tf.placeholder (tf.int32, shape = (None), name = 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope ('DNN_chapter11_8'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer (seed = 1)\n",
    "    hidden1 = tf.layers.dense (X, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name = 'Hidden1')\n",
    "    hidden2 = tf.layers.dense (hidden1, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name = 'Hidden2')\n",
    "    hidden3 = tf.layers.dense (hidden2, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name = 'Hidden3')\n",
    "    hidden4 = tf.layers.dense (hidden3, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name = 'Hidden4')\n",
    "    hidden5 = tf.layers.dense (hidden4, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name = 'Hidden5')\n",
    "    y_predit = tf.layers.dense (hidden5, n_output, kernel_initializer = he_init, name = 'Output')\n",
    "    y_proba = tf.nn.softmax (y_predit, name = \"y_predit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope ('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_predit)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(y_predit, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "n_steps = 20\n",
    "batch_size = 32\n",
    "data_size = X_train.shape[0]\n",
    "n_batch = int (np.ceil (data_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step  0 the train_score is  0.9583333 the validation score is  0.9426 this cost 4.639434814453125\n",
      "At step  1 the train_score is  1.0 the validation score is  0.955 this cost 4.4254233837127686\n",
      "At step  2 the train_score is  1.0 the validation score is  0.9626 this cost 4.510042190551758\n",
      "At step  3 the train_score is  1.0 the validation score is  0.9692 this cost 4.620443105697632\n",
      "At step  4 the train_score is  1.0 the validation score is  0.972 this cost 4.435945272445679\n",
      "At step  5 the train_score is  1.0 the validation score is  0.9748 this cost 4.447862863540649\n",
      "At step  6 the train_score is  1.0 the validation score is  0.977 this cost 4.673173427581787\n",
      "At step  7 the train_score is  1.0 the validation score is  0.9802 this cost 4.44305944442749\n",
      "At step  8 the train_score is  1.0 the validation score is  0.9812 this cost 4.438606023788452\n",
      "At step  9 the train_score is  1.0 the validation score is  0.9822 this cost 4.436694383621216\n",
      "At step  10 the train_score is  1.0 the validation score is  0.9836 this cost 4.4458839893341064\n",
      "At step  11 the train_score is  1.0 the validation score is  0.9856 this cost 4.44385838508606\n",
      "At step  12 the train_score is  1.0 the validation score is  0.988 this cost 4.441618204116821\n",
      "At step  13 the train_score is  1.0 the validation score is  0.989 this cost 4.456693172454834\n",
      "At step  14 the train_score is  1.0 the validation score is  0.9914 this cost 4.703755855560303\n",
      "At step  15 the train_score is  1.0 the validation score is  0.992 this cost 5.3285229206085205\n",
      "At step  16 the train_score is  1.0 the validation score is  0.9934 this cost 5.390258073806763\n",
      "At step  17 the train_score is  1.0 the validation score is  0.994 this cost 5.175909996032715\n",
      "At step  18 the train_score is  1.0 the validation score is  0.9944 this cost 4.579910516738892\n",
      "At step  19 the train_score is  1.0 the validation score is  0.994 this cost 4.622602462768555\n",
      "Run 20 steps total process time is 92.6627037525177\n",
      "the test set accuracy is 0.9737\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    total_start_time = time.time()\n",
    "    for step in range (n_steps):\n",
    "        start_time = time.time ()\n",
    "        for i_batch in range (n_batch):\n",
    "            start = i_batch * batch_size\n",
    "            if i_batch != n_batch - 1:\n",
    "                end = start + batch_size\n",
    "            else:\n",
    "                end = data_size\n",
    "            sess.run (training_op, feed_dict = {X: X_data[start: end], y: y_data[start: end]})\n",
    "        train_score = accuracy.eval (feed_dict = {X: X_data[start: end], y: y_data[start: end]})\n",
    "        validation_score = accuracy.eval (feed_dict = {X: X_validation, y:y_validation})\n",
    "        end_time = time.time ()\n",
    "        run_time = end_time - start_time\n",
    "        print ('At step ', step, 'the train_score is ', train_score, 'the validation score is ', validation_score, 'this cost', run_time)\n",
    "    total_end_time = time.time ()\n",
    "    accuracy_val = accuracy.eval (feed_dict = {X: X_test, y: y_test})\n",
    "    print ('Run', n_steps,  'steps total process time is',  total_end_time - total_start_time)\n",
    "    print ('the test set accuracy is', accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph ()\n",
    "N_feature = X_test.shape[1]\n",
    "n_layers = 5\n",
    "n_neurons = 100\n",
    "learning_rate = 0.01\n",
    "n_output = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam:\n",
    "Step up: 5 layers with 100 nodes on each layer, mini-batch size is 32, train 30 steps.\n",
    "\n",
    "Performance evaluation:\n",
    "\n",
    "    time consumption:  114.90625596046448\n",
    "    performance score:   0.1998\n",
    "\n",
    "Appearently there is an error in decent, it seems moved too fast and missed the lowest point, we shall introduce the earily stop tech.\n",
    "Idealy, at step 15 , you should stop the process. The train score dropped to  0.45833334 at step 15 same as validation score\n",
    "\n",
    "Change the learning rate to 0.001 could resolve this issue, although I still don't understand why "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder (tf.float32, shape = (None, N_feature), name = 'X')\n",
    "y = tf.placeholder (tf.int32, shape = (None), name = 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope ('DNN_chapter11_8'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer (seed = 1)\n",
    "    hidden1 = tf.layers.dense (X, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name = 'Hidden1')\n",
    "    hidden2 = tf.layers.dense (hidden1, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name = 'Hidden2')\n",
    "    hidden3 = tf.layers.dense (hidden2, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name = 'Hidden3')\n",
    "    hidden4 = tf.layers.dense (hidden3, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name = 'Hidden4')\n",
    "    hidden5 = tf.layers.dense (hidden4, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name = 'Hidden5')\n",
    "    y_predit = tf.layers.dense (hidden5, n_output, kernel_initializer = he_init, name = 'Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope ('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_predit)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(y_predit, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "n_steps = 20\n",
    "batch_size = 32\n",
    "data_size = X_train.shape[0]\n",
    "n_batch = int (np.ceil (data_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step  0 the train_score is  0.9583333 the validation score is  0.885 this cost 5.797502040863037\n",
      "0.24410363\n",
      "At step  1 the train_score is  0.20833333 the validation score is  0.1002 this cost 5.729186773300171\n",
      "2.326124\n",
      "At step  2 the train_score is  0.125 the validation score is  0.0986 this cost 5.7704548835754395\n",
      "2.3226068\n",
      "At step  3 the train_score is  0.20833333 the validation score is  0.1002 this cost 5.738136529922485\n",
      "2.3200274\n",
      "At step  4 the train_score is  0.20833333 the validation score is  0.1002 this cost 5.782637119293213\n",
      "2.3207953\n",
      "At step  5 the train_score is  0.20833333 the validation score is  0.1002 this cost 5.783485651016235\n",
      "2.3212085\n",
      "At step  6 the train_score is  0.20833333 the validation score is  0.1002 this cost 5.779216289520264\n",
      "2.3212917\n",
      "At step  7 the train_score is  0.20833333 the validation score is  0.1002 this cost 5.763638257980347\n",
      "2.321304\n",
      "At step  8 the train_score is  0.20833333 the validation score is  0.1002 this cost 5.767236948013306\n",
      "2.3213055\n",
      "At step  9 the train_score is  0.20833333 the validation score is  0.1002 this cost 5.751010417938232\n",
      "2.321306\n",
      "At step  10 the train_score is  0.20833333 the validation score is  0.1002 this cost 6.209610223770142\n",
      "2.321306\n",
      "At step  11 the train_score is  0.20833333 the validation score is  0.1002 this cost 5.775859355926514\n",
      "2.3213058\n",
      "At step  12 the train_score is  0.20833333 the validation score is  0.1002 this cost 5.811497926712036\n",
      "2.3213058\n",
      "At step  13 the train_score is  0.20833333 the validation score is  0.1002 this cost 5.7942585945129395\n",
      "2.321306\n",
      "At step  14 the train_score is  0.20833333 the validation score is  0.1002 this cost 5.7435290813446045\n",
      "2.3213058\n",
      "At step  15 the train_score is  0.20833333 the validation score is  0.1002 this cost 5.935840845108032\n",
      "2.3213058\n",
      "At step  16 the train_score is  0.20833333 the validation score is  0.1002 this cost 5.938317775726318\n",
      "2.3213058\n",
      "At step  17 the train_score is  0.20833333 the validation score is  0.1002 this cost 6.059306621551514\n",
      "2.3213058\n",
      "At step  18 the train_score is  0.20833333 the validation score is  0.1002 this cost 5.923884868621826\n",
      "2.321306\n",
      "At step  19 the train_score is  0.20833333 the validation score is  0.1002 this cost 6.05288290977478\n",
      "2.321306\n",
      "Run 20 steps total process time is 116.912513256073\n",
      "the test set accuracy is 0.0959\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    total_start_time = time.time()\n",
    "    for step in range (n_steps):\n",
    "        start_time = time.time ()\n",
    "        for i_batch in range (n_batch):\n",
    "            start = i_batch * batch_size\n",
    "            if i_batch != n_batch - 1:\n",
    "                end = start + batch_size\n",
    "            else:\n",
    "                end = data_size\n",
    "            sess.run (training_op, feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        loss_train, train_score = sess.run ([loss, accuracy], feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        loss_val, validation_score = sess.run([loss, accuracy], feed_dict = {X: X_validation, y:y_validation})\n",
    "        end_time = time.time ()\n",
    "        run_time = end_time - start_time\n",
    "        print ('At step ', step, 'the train_score is ', train_score, 'the validation score is ', validation_score, 'this cost', run_time)\n",
    "        print (loss_train)\n",
    "    total_end_time = time.time ()\n",
    "    accuracy_val = accuracy.eval (feed_dict = {X: X_test, y: y_test})\n",
    "    print ('Run', n_steps,  'steps total process time is',  total_end_time - total_start_time)\n",
    "    print ('the test set accuracy is', accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam with early stop:\n",
    "\n",
    "Step up: 5 layers with 100 nodes on each layer, mini-batch size is 32, train 20 steps. earily stop if the loss not been reduced further for 35% of the steps.(at step 2, it seems randomly find a good result so stop earily)\n",
    "\n",
    "Performance evaluation:\n",
    "\n",
    "    time consumption: 51.90262746810913\n",
    "    performance score: 0.9\n",
    "    \n",
    "Step up: 5 layers with 100 nodes on each layer, mini-batch size is 32, train 20 steps. earily stop if the loss not been reduced further for 30% of the steps.\n",
    "\n",
    "Performance evaluation:\n",
    "\n",
    "    time consumption: 61.913670778274536\n",
    "    performance score: 0.8656\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step  0 the train_score is  1.0 the validation score is  0.8672 this cost 6.42667293548584\n",
      "At step  0 the loss is  0.5876669 the best loss is  1000 this cost 6.42667293548584\n",
      "At step  1 the train_score is  1.0 the validation score is  0.9408 this cost 6.17248010635376\n",
      "At step  1 the loss is  0.26098397 the best loss is  0.5876669 this cost 6.17248010635376\n",
      "At step  2 the train_score is  1.0 the validation score is  0.913 this cost 6.594958543777466\n",
      "At step  2 the loss is  0.45288447 the best loss is  0.26098397 this cost 6.594958543777466\n",
      "At step  3 the train_score is  1.0 the validation score is  0.9216 this cost 6.328110218048096\n",
      "At step  3 the loss is  0.35236454 the best loss is  0.26098397 this cost 6.328110218048096\n",
      "At step  4 the train_score is  1.0 the validation score is  0.9186 this cost 6.404296398162842\n",
      "At step  4 the loss is  0.44964537 the best loss is  0.26098397 this cost 6.404296398162842\n",
      "At step  5 the train_score is  1.0 the validation score is  0.9086 this cost 6.598033666610718\n",
      "At step  5 the loss is  0.48889396 the best loss is  0.26098397 this cost 6.598033666610718\n",
      "At step  6 the train_score is  0.875 the validation score is  0.7932 this cost 6.550698757171631\n",
      "At step  6 the loss is  1.1180371 the best loss is  0.26098397 this cost 6.550698757171631\n",
      "At step  7 the train_score is  0.9583333 the validation score is  0.8174 this cost 5.86685848236084\n",
      "At step  7 the loss is  0.75360084 the best loss is  0.26098397 this cost 5.86685848236084\n",
      "At step  8 the train_score is  0.9583333 the validation score is  0.7998 this cost 5.895839214324951\n",
      "At step  8 the loss is  1.4297341 the best loss is  0.26098397 this cost 5.895839214324951\n",
      "At step  9 the train_score is  0.7916667 the validation score is  0.8194 this cost 5.866342306137085\n",
      "At step  9 the loss is  0.8059539 the best loss is  0.26098397 this cost 5.866342306137085\n",
      "earily stopped\n",
      "Run 20 steps total process time is 63.093207597732544\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\asiaynrf\\Project\\model_saver\\my_model.ckpt\n",
      "the test set accuracy is 0.9469\n"
     ]
    }
   ],
   "source": [
    "reset_graph ()\n",
    "N_feature = X_test.shape[1]\n",
    "n_layers = 5\n",
    "n_neurons = 100\n",
    "learning_rate = 0.01\n",
    "n_output = 10\n",
    "\n",
    "X = tf.placeholder (tf.float32, shape = (None, N_feature), name = 'X')\n",
    "y = tf.placeholder (tf.int32, shape = (None), name = 'y')\n",
    "\n",
    "with tf.name_scope ('DNN_chapter11_8'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer (seed = 1)\n",
    "    hidden1 = tf.layers.dense (X, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name = 'Hidden1')\n",
    "    hidden2 = tf.layers.dense (hidden1, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name = 'Hidden2')\n",
    "    hidden3 = tf.layers.dense (hidden2, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name = 'Hidden3')\n",
    "    hidden4 = tf.layers.dense (hidden3, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name = 'Hidden4')\n",
    "    hidden5 = tf.layers.dense (hidden4, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name = 'Hidden5')\n",
    "    y_predit = tf.layers.dense (hidden5, n_output, kernel_initializer = he_init, name = 'Output')\n",
    "\n",
    "with tf.name_scope ('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_predit)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(y_predit, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_steps = 20\n",
    "batch_size = 32\n",
    "data_size = X_train.shape[0]\n",
    "n_batch = int (np.ceil (data_size / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    total_start_time = time.time()\n",
    "    best_loss = 1000 \n",
    "    without_update_counter = 0\n",
    "    max_without_update = int (0.35 * n_steps)\n",
    "    for step in range (n_steps):\n",
    "        start_time = time.time ()\n",
    "        for i_batch in range (n_batch):\n",
    "            start = i_batch * batch_size\n",
    "            if i_batch != n_batch - 1:\n",
    "                end = start + batch_size\n",
    "            else:\n",
    "                end = data_size\n",
    "            sess.run (training_op, feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        train_score = accuracy.eval (feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        loss_val, validation_score = sess.run([loss, accuracy], feed_dict = {X: X_validation, y:y_validation})\n",
    "        end_time = time.time ()\n",
    "        run_time = end_time - start_time\n",
    "        print ('At step ', step, 'the train_score is ', train_score, 'the validation score is ', validation_score, 'this cost', run_time)\n",
    "        print ('At step ', step, 'the loss is ', loss_val, 'the best loss is ', best_loss, 'this cost', run_time)\n",
    "        if loss_val < best_loss:\n",
    "            save_path = saver.save (sess, r'C:\\Users\\asiaynrf\\Project\\model_saver\\my_model.ckpt')\n",
    "            best_loss = loss_val\n",
    "            without_update_counter = 0\n",
    "        else:\n",
    "            without_update_counter += 1\n",
    "            if without_update_counter > max_without_update:\n",
    "                print ('earily stopped')\n",
    "                break\n",
    "    total_end_time = time.time ()\n",
    "    print ('Run', n_steps,  'steps total process time is',  total_end_time - total_start_time)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore (sess, r'C:\\Users\\asiaynrf\\Project\\model_saver\\my_model.ckpt')\n",
    "    accuracy_val = accuracy.eval (feed_dict = {X: X_test, y: y_test})\n",
    "    print ('the test set accuracy is', accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GD without mini-batch\n",
    "Performance evaluation: \n",
    "\n",
    "Score: (fogot to run the test set) 2000 steps's validation score is  0.9448. For the previous setup, it is 32 batch size, in another word, one step is an 1719 steps, so 20 step is 34380 steps in total. No wonder why mini-batch is better.\n",
    "\n",
    "Take look at the time consumption, it is 1468s, compare to 120s in previous method to have similar (actually worse) performance. \n",
    "\n",
    "time consume: 1483.5600037574768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_graph()\n",
    "\n",
    "# X = tf.placeholder (tf.float32, shape = (None, N_feature), name = 'X')\n",
    "# y = tf.placeholder (tf.int32, shape = (None), name = 'y')\n",
    "\n",
    "# with tf.name_scope ('DNN_chapter11_8'):\n",
    "#     he_init = tf.contrib.layers.variance_scaling_initializer (seed = 1)\n",
    "#     hidden1 = tf.layers.dense (X, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name = 'Hidden1')\n",
    "#     hidden2 = tf.layers.dense (hidden1, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name = 'Hidden2')\n",
    "#     hidden3 = tf.layers.dense (hidden2, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name = 'Hidden3')\n",
    "#     hidden4 = tf.layers.dense (hidden3, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name = 'Hidden4')\n",
    "#     hidden5 = tf.layers.dense (hidden4, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name = 'Hidden5')\n",
    "#     y_predit = tf.layers.dense (hidden5, n_output, kernel_initializer = he_init, name = 'Output')\n",
    "\n",
    "# with tf.name_scope ('loss'):\n",
    "#     xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_predit)\n",
    "#     loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "# with tf.name_scope(\"train\"):\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#     training_op = optimizer.minimize(loss)\n",
    "\n",
    "# with tf.name_scope(\"eval\"):\n",
    "#     correct = tf.nn.in_top_k(y_predit, y, 1)\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "# saver = tf.train.Saver()\n",
    "# n_steps = 2000\n",
    "# batch_size = 32\n",
    "# data_size = X_train.shape[0]\n",
    "# n_batch = int (np.ceil (data_size / batch_size))\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     init.run()\n",
    "#     total_start_time = time.time()\n",
    "#     best_loss = 1000 \n",
    "#     without_update_counter = 0\n",
    "#     max_without_update = int (0.2 * n_steps)\n",
    "#     for step in range (n_steps):\n",
    "#         start_time = time.time ()\n",
    "#         sess.run (training_op, feed_dict = {X: X_train, y: y_train})\n",
    "#         train_score = accuracy.eval (feed_dict = {X: X_train, y: y_train})\n",
    "#         loss_val, validation_score = sess.run([loss, accuracy], feed_dict = {X: X_validation, y:y_validation})\n",
    "#         end_time = time.time ()\n",
    "#         run_time = end_time - start_time\n",
    "#         print ('At step ', step, 'the train_score is ', train_score, 'the validation score is ', validation_score, 'this cost', run_time)\n",
    "#         # print ('At step ', step, 'the loss is ', loss_val, 'the best loss is ', best_loss, 'this cost', run_time)\n",
    "# #         if loss_val < best_loss:\n",
    "# #             save_path = saver.save (sess, r'C:\\Users\\asiaynrf\\Project\\model_saver\\my_model.ckpt')\n",
    "# #             best_loss = loss_val\n",
    "# #             without_update_counter = 0\n",
    "# #         else:\n",
    "# #             without_update_counter += 1\n",
    "# #             if without_update_counter > max_without_update:\n",
    "# #                 print ('earily stopped')\n",
    "# #                 break\n",
    "#     total_end_time = time.time ()\n",
    "#     print ('Run', n_steps,  'steps total process time is',  total_end_time - total_start_time)\n",
    "#     accuracy_val = accuracy.eval (feed_dict = {X: X_test, y: y_test})\n",
    "#     print ('the test set accuracy is', accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    run time 160.15829229354858\n",
    "    score :  0.9071\n",
    "if take a look, the converge is geting much slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step  0 the train_score is  0.7083333 the validation score is  0.6438 this cost 8.860485792160034\n",
      "At step  1 the train_score is  0.7083333 the validation score is  0.6846 this cost 8.325199127197266\n",
      "At step  2 the train_score is  0.7916667 the validation score is  0.7202 this cost 8.837181806564331\n",
      "At step  3 the train_score is  0.8333333 the validation score is  0.7434 this cost 8.532508373260498\n",
      "At step  4 the train_score is  0.875 the validation score is  0.7686 this cost 8.110628366470337\n",
      "At step  5 the train_score is  0.9166667 the validation score is  0.7892 this cost 8.592307329177856\n",
      "At step  6 the train_score is  0.9166667 the validation score is  0.8064 this cost 8.850103616714478\n",
      "At step  7 the train_score is  0.9166667 the validation score is  0.8194 this cost 9.84930157661438\n",
      "At step  8 the train_score is  0.9166667 the validation score is  0.828 this cost 9.105423927307129\n",
      "At step  9 the train_score is  0.9166667 the validation score is  0.8334 this cost 9.221487522125244\n",
      "At step  10 the train_score is  0.9166667 the validation score is  0.8414 this cost 8.614898681640625\n",
      "At step  11 the train_score is  0.9166667 the validation score is  0.8458 this cost 8.91478681564331\n",
      "At step  12 the train_score is  0.9166667 the validation score is  0.8498 this cost 9.161916494369507\n",
      "At step  13 the train_score is  0.9166667 the validation score is  0.8542 this cost 8.318667650222778\n",
      "At step  14 the train_score is  0.9166667 the validation score is  0.858 this cost 8.789518356323242\n",
      "At step  15 the train_score is  0.9166667 the validation score is  0.8602 this cost 8.699188470840454\n",
      "At step  16 the train_score is  0.9583333 the validation score is  0.8646 this cost 9.179411888122559\n",
      "At step  17 the train_score is  0.9583333 the validation score is  0.8674 this cost 8.828520059585571\n",
      "At step  18 the train_score is  0.9583333 the validation score is  0.8702 this cost 8.524041652679443\n",
      "At step  19 the train_score is  0.9583333 the validation score is  0.8724 this cost 9.176499843597412\n",
      "Run 20 steps total process time is 176.4960949420929\n",
      "the test set accuracy is 0.8733\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "X = tf.placeholder (tf.float32, shape = (None, N_feature), name = 'X')\n",
    "y = tf.placeholder (tf.int32, shape = (None), name = 'y')\n",
    "\n",
    "with tf.name_scope ('DNN_chapter11_8'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer (seed = 1)\n",
    "    training = tf.placeholder_with_default (False, shape = (), name = 'training')\n",
    "    Batch_norm_layer = partial (tf.layers.batch_normalization, training = training, momentum = 0.9)\n",
    "    hidden1 = tf.layers.dense (X, n_neurons, kernel_initializer = he_init, name = 'Hidden1')\n",
    "    batch_h1 = Batch_norm_layer (hidden1)\n",
    "    batch_h1_act = tf.nn.elu (batch_h1)\n",
    "    hidden2 = tf.layers.dense (batch_h1_act, n_neurons, kernel_initializer = he_init, name = 'Hidden2')\n",
    "    batch_h2 = Batch_norm_layer (hidden2)\n",
    "    batch_h2_act = tf.nn.elu (batch_h2)\n",
    "    hidden3 = tf.layers.dense (batch_h2_act, n_neurons, kernel_initializer = he_init, name = 'Hidden3')\n",
    "    batch_h3 = Batch_norm_layer (hidden3)\n",
    "    batch_h3_act = tf.nn.elu (batch_h3)\n",
    "    hidden4 = tf.layers.dense (batch_h3_act, n_neurons, kernel_initializer = he_init, name = 'Hidden4')\n",
    "    batch_h4 = Batch_norm_layer (hidden4)\n",
    "    batch_h4_act = tf.nn.elu (batch_h4)\n",
    "    hidden5 = tf.layers.dense (batch_h4_act, n_neurons, kernel_initializer = he_init, name = 'Hidden5')\n",
    "    batch_h5 = Batch_norm_layer (hidden5)\n",
    "    batch_h5_act = tf.nn.elu (batch_h5)\n",
    "    y_predit_before_norm = tf.layers.dense (batch_h5_act, n_output, kernel_initializer = he_init, name = 'Output')\n",
    "    y_predit = Batch_norm_layer (y_predit_before_norm)\n",
    "\n",
    "with tf.name_scope ('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_predit)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(y_predit, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "extra_update_ops = tf.get_collection (tf.GraphKeys.UPDATE_OPS)\n",
    "n_steps = 20\n",
    "batch_size = 32\n",
    "data_size = X_train.shape[0]\n",
    "n_batch = int (np.ceil (data_size / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    total_start_time = time.time()\n",
    "    for step in range (n_steps):\n",
    "        start_time = time.time ()\n",
    "        for i_batch in range (n_batch):\n",
    "            start = i_batch * batch_size\n",
    "            if i_batch != n_batch - 1:\n",
    "                end = start + batch_size\n",
    "            else:\n",
    "                end = data_size\n",
    "            sess.run ([training_op, training], feed_dict = {training: True , X: X_train[start: end], y: y_train[start: end]})\n",
    "        train_score = accuracy.eval (feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        loss_val, validation_score = sess.run([loss, accuracy], feed_dict = {X: X_validation, y:y_validation})\n",
    "        end_time = time.time ()\n",
    "        run_time = end_time - start_time\n",
    "        print ('At step ', step, 'the train_score is ', train_score, 'the validation score is ', validation_score, 'this cost', run_time)\n",
    "    total_end_time = time.time ()\n",
    "    print ('Run', n_steps,  'steps total process time is',  total_end_time - total_start_time)\n",
    "    accuracy_val = accuracy.eval (feed_dict = {X: X_test, y: y_test})\n",
    "    print ('the test set accuracy is', accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add gradiant clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The gradiant clip applied at training op not inside of DNN\n",
    " \n",
    "     Performance: 0.9735\n",
    "     Time comume: 89.36819243431091"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph ()\n",
    "\n",
    "import time\n",
    "N_feature = X_test.shape[1]\n",
    "n_layers = 5\n",
    "n_neurons = 100\n",
    "learning_rate = 0.01\n",
    "n_output = 10\n",
    "clip_threshold = 1\n",
    "\n",
    "X = tf.placeholder (tf.float32, shape = (None, N_feature), name = 'X')\n",
    "y = tf.placeholder (tf.int32, shape = (None), name = 'y')\n",
    "\n",
    "with tf.name_scope ('DNN_chapter11_8'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer (seed = 1)\n",
    "    inputs = X\n",
    "    for layer in range (n_layers):\n",
    "        inputs = tf.layers.dense (inputs, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name=\"hidden%d\" % (layer + 1))\n",
    "    y_predit = tf.layers.dense (inputs, n_output, kernel_initializer = he_init, name = 'output')\n",
    "    y_proba = tf.nn.softmax(y_predit, name=\"Y_proba\")\n",
    "\n",
    "with tf.name_scope ('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_predit)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    grads_and_vars  = optimizer.compute_gradients (loss)\n",
    "    capped_grads_and_vars = [(tf.clip_by_value(g,-clip_threshold, clip_threshold), v) for g,v in grads_and_vars]\n",
    "    training_op = optimizer.apply_gradients (capped_grads_and_vars)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(y_predit, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "extra_update_ops = tf.get_collection (tf.GraphKeys.UPDATE_OPS)\n",
    "n_steps = 20\n",
    "batch_size = 32\n",
    "data_size = X_train.shape[0]\n",
    "n_batch = int (np.ceil (data_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step  0 the train_score is  1.0 the validation score is  0.9356 this cost 4.768087387084961\n",
      "At step  1 the train_score is  1.0 the validation score is  0.952 this cost 4.486506223678589\n",
      "At step  2 the train_score is  1.0 the validation score is  0.9586 this cost 4.470193147659302\n",
      "At step  3 the train_score is  1.0 the validation score is  0.9634 this cost 4.464956045150757\n",
      "At step  4 the train_score is  1.0 the validation score is  0.9674 this cost 4.529657602310181\n",
      "At step  5 the train_score is  1.0 the validation score is  0.969 this cost 4.511454343795776\n",
      "At step  6 the train_score is  1.0 the validation score is  0.9698 this cost 4.470094442367554\n",
      "At step  7 the train_score is  1.0 the validation score is  0.97 this cost 4.475607633590698\n",
      "At step  8 the train_score is  1.0 the validation score is  0.9704 this cost 4.454093933105469\n",
      "At step  9 the train_score is  1.0 the validation score is  0.9706 this cost 4.4885993003845215\n",
      "At step  10 the train_score is  1.0 the validation score is  0.9712 this cost 4.485699892044067\n",
      "At step  11 the train_score is  1.0 the validation score is  0.9714 this cost 4.4910948276519775\n",
      "At step  12 the train_score is  1.0 the validation score is  0.9734 this cost 4.461049556732178\n",
      "At step  13 the train_score is  1.0 the validation score is  0.9742 this cost 4.4641547203063965\n",
      "At step  14 the train_score is  1.0 the validation score is  0.9752 this cost 4.472657680511475\n",
      "At step  15 the train_score is  1.0 the validation score is  0.9748 this cost 4.481669902801514\n",
      "At step  16 the train_score is  1.0 the validation score is  0.9752 this cost 4.477653980255127\n",
      "At step  17 the train_score is  1.0 the validation score is  0.9752 this cost 4.472247123718262\n",
      "At step  18 the train_score is  1.0 the validation score is  0.9762 this cost 4.489262104034424\n",
      "At step  19 the train_score is  1.0 the validation score is  0.9758 this cost 4.4847800731658936\n",
      "Run 20 steps total process time is 89.90252900123596\n",
      "the test set accuracy is 0.97\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    total_start_time = time.time()\n",
    "    for step in range (n_steps):\n",
    "        start_time = time.time ()\n",
    "        for i_batch in range (n_batch):\n",
    "            start = i_batch * batch_size\n",
    "            if i_batch != n_batch - 1:\n",
    "                end = start + batch_size\n",
    "            else:\n",
    "                end = data_size\n",
    "            sess.run (training_op, feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        train_score = accuracy.eval (feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        loss_val, validation_score = sess.run([loss, accuracy], feed_dict = {X: X_validation, y:y_validation})\n",
    "        end_time = time.time ()\n",
    "        run_time = end_time - start_time\n",
    "        print ('At step ', step, 'the train_score is ', train_score, 'the validation score is ', validation_score, 'this cost', run_time)\n",
    "    total_end_time = time.time ()\n",
    "    print ('Run', n_steps,  'steps total process time is',  total_end_time - total_start_time)\n",
    "    accuracy_val = accuracy.eval (feed_dict = {X: X_test, y: y_test})\n",
    "    print ('the test set accuracy is', accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "N_feature = X_test.shape[1]\n",
    "n_layers = 5\n",
    "n_neurons = 100\n",
    "learning_rate = 0.01\n",
    "n_output = 10\n",
    "clip_threshold = 1\n",
    "dropout_rate = 0.5\n",
    "\n",
    "X = tf.placeholder (tf.float32, shape = (None, N_feature), name = 'X')\n",
    "y = tf.placeholder (tf.int32, shape = (None), name = 'y')\n",
    "training = tf.placeholder_with_default (False, shape = (), name = 'Training_indicator')\n",
    "\n",
    "with tf.name_scope ('DNN_chapter11_8'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer (seed = 1)\n",
    "    X_dropped = tf.layers.dropout (X, rate = dropout_rate, training = training)\n",
    "    inputs = X_dropped\n",
    "    for layer in range (n_layers):\n",
    "        inputs = tf.layers.dense (inputs, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name=\"hidden%d\" % (layer + 1))\n",
    "        inputs = tf.layers.dropout (inputs, rate = dropout_rate, training = training)\n",
    "    y_predit = tf.layers.dense (inputs, n_output, kernel_initializer = he_init, name = 'output')\n",
    "    y_proba = tf.nn.softmax(y_predit, name=\"Y_proba\")\n",
    "\n",
    "with tf.name_scope ('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_predit)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    grads_and_vars  = optimizer.compute_gradients (loss)\n",
    "    capped_grads_and_vars = [(tf.clip_by_value(g,-clip_threshold, clip_threshold), v) for g,v in grads_and_vars]\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(y_predit, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "extra_update_ops = tf.get_collection (tf.GraphKeys.UPDATE_OPS)\n",
    "n_steps = 20\n",
    "batch_size = 32\n",
    "data_size = X_train.shape[0]\n",
    "n_batch = int (np.ceil (data_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step  0 the train_score is  0.6666667 the validation score is  0.6528 this cost 5.542396783828735\n",
      "At step  1 the train_score is  0.875 the validation score is  0.7806 this cost 5.366839647293091\n",
      "At step  2 the train_score is  0.875 the validation score is  0.7954 this cost 5.343973875045776\n",
      "At step  3 the train_score is  0.875 the validation score is  0.8064 this cost 5.36245322227478\n",
      "At step  4 the train_score is  0.9583333 the validation score is  0.8488 this cost 5.3312554359436035\n",
      "At step  5 the train_score is  0.9583333 the validation score is  0.861 this cost 5.351331472396851\n",
      "At step  6 the train_score is  0.9583333 the validation score is  0.8724 this cost 5.349643230438232\n",
      "At step  7 the train_score is  0.9583333 the validation score is  0.852 this cost 5.3778181076049805\n",
      "At step  8 the train_score is  0.9583333 the validation score is  0.8708 this cost 5.343310832977295\n",
      "At step  9 the train_score is  0.9583333 the validation score is  0.8844 this cost 5.374162435531616\n",
      "At step  10 the train_score is  0.9583333 the validation score is  0.8828 this cost 5.356730222702026\n",
      "At step  11 the train_score is  0.9583333 the validation score is  0.894 this cost 5.331351280212402\n",
      "At step  12 the train_score is  1.0 the validation score is  0.8966 this cost 5.334408521652222\n",
      "At step  13 the train_score is  1.0 the validation score is  0.8998 this cost 5.3338987827301025\n",
      "At step  14 the train_score is  1.0 the validation score is  0.9036 this cost 5.3534369468688965\n",
      "At step  15 the train_score is  1.0 the validation score is  0.9042 this cost 5.337182998657227\n",
      "At step  16 the train_score is  1.0 the validation score is  0.9074 this cost 5.339731693267822\n",
      "At step  17 the train_score is  1.0 the validation score is  0.9076 this cost 5.335555791854858\n",
      "At step  18 the train_score is  1.0 the validation score is  0.9068 this cost 5.400794267654419\n",
      "At step  19 the train_score is  1.0 the validation score is  0.912 this cost 5.385401964187622\n",
      "Run 20 steps total process time is 107.25569725036621\n",
      "the test set accuracy is 0.9061\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    total_start_time = time.time()\n",
    "    for step in range (n_steps):\n",
    "        start_time = time.time ()\n",
    "        for i_batch in range (n_batch):\n",
    "            start = i_batch * batch_size\n",
    "            if i_batch != n_batch - 1:\n",
    "                end = start + batch_size\n",
    "            else:\n",
    "                end = data_size\n",
    "            sess.run (training_op, feed_dict = {training: True, X: X_train[start: end], y: y_train[start: end]})\n",
    "        train_score = accuracy.eval (feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        loss_val, validation_score = sess.run([loss, accuracy], feed_dict = {X: X_validation, y:y_validation})\n",
    "        end_time = time.time ()\n",
    "        run_time = end_time - start_time\n",
    "        print ('At step ', step, 'the train_score is ', train_score, 'the validation score is ', validation_score, 'this cost', run_time)\n",
    "    total_end_time = time.time ()\n",
    "    print ('Run', n_steps,  'steps total process time is',  total_end_time - total_start_time)\n",
    "    accuracy_val = accuracy.eval (feed_dict = {X: X_test, y: y_test})\n",
    "    print ('the test set accuracy is', accuracy_val)\n",
    "    writer = tf.summary.FileWriter(r\"C:\\Users\\asiaynrf\\Desktop\\fun\\handson_ML\\Graph_saver\", sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum with momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph ()\n",
    "\n",
    "import time\n",
    "N_feature = X_test.shape[1]\n",
    "n_layers = 5\n",
    "n_neurons = 100\n",
    "learning_rate = 0.01\n",
    "n_output = 10\n",
    "clip_threshold = 1\n",
    "momentum_value = 0.9\n",
    "\n",
    "X = tf.placeholder (tf.float32, shape = (None, N_feature), name = 'X')\n",
    "y = tf.placeholder (tf.int32, shape = (None), name = 'y')\n",
    "\n",
    "with tf.name_scope ('DNN_chapter11_8'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer (seed = 1)\n",
    "    inputs = X\n",
    "    for layer in range (n_layers):\n",
    "        inputs = tf.layers.dense (inputs, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name=\"hidden%d\" % (layer + 1))\n",
    "    y_predit = tf.layers.dense (inputs, n_output, kernel_initializer = he_init, name = 'output')\n",
    "    y_proba = tf.nn.softmax(y_predit, name=\"Y_proba\")\n",
    "\n",
    "with tf.name_scope ('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_predit)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate, momentum = momentum_value)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(y_predit, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "n_steps = 20\n",
    "batch_size = 32\n",
    "data_size = X_train.shape[0]\n",
    "n_batch = int (np.ceil (data_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step  0 the train_score is  1.0 the validation score is  0.9484 this cost 5.273808002471924\n",
      "At step  1 the train_score is  1.0 the validation score is  0.9644 this cost 5.7890400886535645\n",
      "At step  2 the train_score is  1.0 the validation score is  0.9658 this cost 4.836617708206177\n",
      "At step  3 the train_score is  1.0 the validation score is  0.9684 this cost 4.849977493286133\n",
      "At step  4 the train_score is  1.0 the validation score is  0.965 this cost 4.846926689147949\n",
      "At step  5 the train_score is  1.0 the validation score is  0.9696 this cost 4.839024305343628\n",
      "At step  6 the train_score is  1.0 the validation score is  0.97 this cost 4.955053329467773\n",
      "At step  7 the train_score is  1.0 the validation score is  0.9776 this cost 4.829431056976318\n",
      "At step  8 the train_score is  1.0 the validation score is  0.9764 this cost 4.834311008453369\n",
      "At step  9 the train_score is  1.0 the validation score is  0.978 this cost 4.828329086303711\n",
      "At step  10 the train_score is  1.0 the validation score is  0.974 this cost 4.826342582702637\n",
      "At step  11 the train_score is  1.0 the validation score is  0.9788 this cost 4.825108766555786\n",
      "At step  12 the train_score is  1.0 the validation score is  0.9754 this cost 4.873437404632568\n",
      "At step  13 the train_score is  1.0 the validation score is  0.978 this cost 4.850415468215942\n",
      "At step  14 the train_score is  1.0 the validation score is  0.9804 this cost 4.831103086471558\n",
      "At step  15 the train_score is  1.0 the validation score is  0.9784 this cost 4.828019618988037\n",
      "At step  16 the train_score is  1.0 the validation score is  0.9748 this cost 4.837512254714966\n",
      "At step  17 the train_score is  1.0 the validation score is  0.979 this cost 5.534175395965576\n",
      "At step  18 the train_score is  1.0 the validation score is  0.9764 this cost 5.606215238571167\n",
      "At step  19 the train_score is  1.0 the validation score is  0.9814 this cost 4.825460433959961\n",
      "Run 20 steps total process time is 99.72936153411865\n",
      "the test set accuracy is 0.9792\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    total_start_time = time.time()\n",
    "    for step in range (n_steps):\n",
    "        start_time = time.time ()\n",
    "        for i_batch in range (n_batch):\n",
    "            start = i_batch * batch_size\n",
    "            if i_batch != n_batch - 1:\n",
    "                end = start + batch_size\n",
    "            else:\n",
    "                end = data_size\n",
    "            sess.run (training_op, feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        train_score = accuracy.eval (feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        loss_val, validation_score = sess.run([loss, accuracy], feed_dict = {X: X_validation, y:y_validation})\n",
    "        end_time = time.time ()\n",
    "        run_time = end_time - start_time\n",
    "        print ('At step ', step, 'the train_score is ', train_score, 'the validation score is ', validation_score, 'this cost', run_time)\n",
    "    total_end_time = time.time ()\n",
    "    print ('Run', n_steps,  'steps total process time is',  total_end_time - total_start_time)\n",
    "    accuracy_val = accuracy.eval (feed_dict = {X: X_test, y: y_test})\n",
    "    print ('the test set accuracy is', accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer: AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph ()\n",
    "\n",
    "import time\n",
    "N_feature = X_test.shape[1]\n",
    "n_layers = 5\n",
    "n_neurons = 100\n",
    "learning_rate = 0.01\n",
    "n_output = 10\n",
    "clip_threshold = 1\n",
    "momentum_value = 0.9\n",
    "\n",
    "X = tf.placeholder (tf.float32, shape = (None, N_feature), name = 'X')\n",
    "y = tf.placeholder (tf.int32, shape = (None), name = 'y')\n",
    "\n",
    "with tf.name_scope ('DNN_chapter11_8'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer (seed = 1)\n",
    "    inputs = X\n",
    "    for layer in range (n_layers):\n",
    "        inputs = tf.layers.dense (inputs, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name=\"hidden%d\" % (layer + 1))\n",
    "    y_predit = tf.layers.dense (inputs, n_output, kernel_initializer = he_init, name = 'output')\n",
    "    y_proba = tf.nn.softmax(y_predit, name=\"Y_proba\")\n",
    "\n",
    "with tf.name_scope ('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_predit)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate = learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(y_predit, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "n_steps = 20\n",
    "batch_size = 32\n",
    "data_size = X_train.shape[0]\n",
    "n_batch = int (np.ceil (data_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step  0 the train_score is  1.0 the validation score is  0.9526 this cost 5.174942255020142\n",
      "At step  1 the train_score is  1.0 the validation score is  0.9628 this cost 5.226587772369385\n",
      "At step  2 the train_score is  1.0 the validation score is  0.968 this cost 5.039770126342773\n",
      "At step  3 the train_score is  1.0 the validation score is  0.9698 this cost 4.967498540878296\n",
      "At step  4 the train_score is  1.0 the validation score is  0.972 this cost 5.003436803817749\n",
      "At step  5 the train_score is  1.0 the validation score is  0.9734 this cost 5.06840968132019\n",
      "At step  6 the train_score is  1.0 the validation score is  0.9746 this cost 5.402022123336792\n",
      "At step  7 the train_score is  1.0 the validation score is  0.975 this cost 5.40156888961792\n",
      "At step  8 the train_score is  1.0 the validation score is  0.9756 this cost 5.0783350467681885\n",
      "At step  9 the train_score is  1.0 the validation score is  0.9758 this cost 4.975041627883911\n",
      "At step  10 the train_score is  1.0 the validation score is  0.976 this cost 5.028276205062866\n",
      "At step  11 the train_score is  1.0 the validation score is  0.976 this cost 5.070950031280518\n",
      "At step  12 the train_score is  1.0 the validation score is  0.9768 this cost 5.264193296432495\n",
      "At step  13 the train_score is  1.0 the validation score is  0.9772 this cost 5.014462947845459\n",
      "At step  14 the train_score is  1.0 the validation score is  0.977 this cost 5.071137428283691\n",
      "At step  15 the train_score is  1.0 the validation score is  0.9766 this cost 5.104431390762329\n",
      "At step  16 the train_score is  1.0 the validation score is  0.977 this cost 5.088073015213013\n",
      "At step  17 the train_score is  1.0 the validation score is  0.977 this cost 5.047852993011475\n",
      "At step  18 the train_score is  1.0 the validation score is  0.977 this cost 5.038042306900024\n",
      "At step  19 the train_score is  1.0 the validation score is  0.9772 this cost 5.113534688949585\n",
      "Run 20 steps total process time is 102.18058776855469\n",
      "the test set accuracy is 0.9748\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    total_start_time = time.time()\n",
    "    for step in range (n_steps):\n",
    "        start_time = time.time ()\n",
    "        for i_batch in range (n_batch):\n",
    "            start = i_batch * batch_size\n",
    "            if i_batch != n_batch - 1:\n",
    "                end = start + batch_size\n",
    "            else:\n",
    "                end = data_size\n",
    "            sess.run (training_op, feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        train_score = accuracy.eval (feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        loss_val, validation_score = sess.run([loss, accuracy], feed_dict = {X: X_validation, y:y_validation})\n",
    "        end_time = time.time ()\n",
    "        run_time = end_time - start_time\n",
    "        print ('At step ', step, 'the train_score is ', train_score, 'the validation score is ', validation_score, 'this cost', run_time)\n",
    "    total_end_time = time.time ()\n",
    "    print ('Run', n_steps,  'steps total process time is',  total_end_time - total_start_time)\n",
    "    accuracy_val = accuracy.eval (feed_dict = {X: X_test, y: y_test})\n",
    "    print ('the test set accuracy is', accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer: RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph ()\n",
    "\n",
    "import time\n",
    "N_feature = X_test.shape[1]\n",
    "n_layers = 5\n",
    "n_neurons = 100\n",
    "learning_rate = 0.01\n",
    "n_output = 10\n",
    "clip_threshold = 1\n",
    "momentum_value = 0.9\n",
    "\n",
    "X = tf.placeholder (tf.float32, shape = (None, N_feature), name = 'X')\n",
    "y = tf.placeholder (tf.int32, shape = (None), name = 'y')\n",
    "\n",
    "with tf.name_scope ('DNN_chapter11_8'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer (seed = 1)\n",
    "    inputs = X\n",
    "    for layer in range (n_layers):\n",
    "        inputs = tf.layers.dense (inputs, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name=\"hidden%d\" % (layer + 1))\n",
    "    y_predit = tf.layers.dense (inputs, n_output, kernel_initializer = he_init, name = 'output')\n",
    "    y_proba = tf.nn.softmax(y_predit, name=\"Y_proba\")\n",
    "\n",
    "with tf.name_scope ('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_predit)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate = learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(y_predit, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "n_steps = 20\n",
    "batch_size = 32\n",
    "data_size = X_train.shape[0]\n",
    "n_batch = int (np.ceil (data_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step  0 the train_score is  1.0 the validation score is  0.9314 this cost 5.789090633392334\n",
      "At step  1 the train_score is  1.0 the validation score is  0.9376 this cost 5.8565919399261475\n",
      "At step  2 the train_score is  1.0 the validation score is  0.9268 this cost 5.766861438751221\n",
      "At step  3 the train_score is  1.0 the validation score is  0.926 this cost 5.696022033691406\n",
      "At step  4 the train_score is  1.0 the validation score is  0.9306 this cost 5.832495450973511\n",
      "At step  5 the train_score is  1.0 the validation score is  0.927 this cost 5.9576849937438965\n",
      "At step  6 the train_score is  1.0 the validation score is  0.9404 this cost 5.83569073677063\n",
      "At step  7 the train_score is  1.0 the validation score is  0.9274 this cost 5.798902273178101\n",
      "At step  8 the train_score is  0.9583333 the validation score is  0.8946 this cost 5.5825793743133545\n",
      "At step  9 the train_score is  1.0 the validation score is  0.9248 this cost 5.615818977355957\n",
      "At step  10 the train_score is  1.0 the validation score is  0.9386 this cost 5.600684404373169\n",
      "At step  11 the train_score is  0.9583333 the validation score is  0.8864 this cost 5.6195409297943115\n",
      "At step  12 the train_score is  1.0 the validation score is  0.9426 this cost 5.610934734344482\n",
      "At step  13 the train_score is  1.0 the validation score is  0.9034 this cost 5.602332830429077\n",
      "At step  14 the train_score is  1.0 the validation score is  0.8814 this cost 5.631208419799805\n",
      "At step  15 the train_score is  1.0 the validation score is  0.9096 this cost 6.77872109413147\n",
      "At step  16 the train_score is  1.0 the validation score is  0.913 this cost 5.871881723403931\n",
      "At step  17 the train_score is  0.9166667 the validation score is  0.8208 this cost 5.714092969894409\n",
      "At step  18 the train_score is  1.0 the validation score is  0.8092 this cost 6.012648105621338\n",
      "At step  19 the train_score is  0.9166667 the validation score is  0.7844 this cost 5.872249603271484\n",
      "Run 20 steps total process time is 116.05007433891296\n",
      "the test set accuracy is 0.787\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    total_start_time = time.time()\n",
    "    for step in range (n_steps):\n",
    "        start_time = time.time ()\n",
    "        for i_batch in range (n_batch):\n",
    "            start = i_batch * batch_size\n",
    "            if i_batch != n_batch - 1:\n",
    "                end = start + batch_size\n",
    "            else:\n",
    "                end = data_size\n",
    "            sess.run (training_op, feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        train_score = accuracy.eval (feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        loss_val, validation_score = sess.run([loss, accuracy], feed_dict = {X: X_validation, y:y_validation})\n",
    "        end_time = time.time ()\n",
    "        run_time = end_time - start_time\n",
    "        print ('At step ', step, 'the train_score is ', train_score, 'the validation score is ', validation_score, 'this cost', run_time)\n",
    "    total_end_time = time.time ()\n",
    "    print ('Run', n_steps,  'steps total process time is',  total_end_time - total_start_time)\n",
    "    accuracy_val = accuracy.eval (feed_dict = {X: X_test, y: y_test})\n",
    "    print ('the test set accuracy is', accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer: AdadeltaOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph ()\n",
    "\n",
    "import time\n",
    "N_feature = X_test.shape[1]\n",
    "n_layers = 5\n",
    "n_neurons = 100\n",
    "learning_rate = 0.01\n",
    "n_output = 10\n",
    "clip_threshold = 1\n",
    "momentum_value = 0.9\n",
    "\n",
    "X = tf.placeholder (tf.float32, shape = (None, N_feature), name = 'X')\n",
    "y = tf.placeholder (tf.int32, shape = (None), name = 'y')\n",
    "\n",
    "with tf.name_scope ('DNN_chapter11_8'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer (seed = 1)\n",
    "    inputs = X\n",
    "    for layer in range (n_layers):\n",
    "        inputs = tf.layers.dense (inputs, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name=\"hidden%d\" % (layer + 1))\n",
    "    y_predit = tf.layers.dense (inputs, n_output, kernel_initializer = he_init, name = 'output')\n",
    "    y_proba = tf.nn.softmax(y_predit, name=\"Y_proba\")\n",
    "\n",
    "with tf.name_scope ('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_predit)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate = learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(y_predit, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "n_steps = 20\n",
    "batch_size = 32\n",
    "data_size = X_train.shape[0]\n",
    "n_batch = int (np.ceil (data_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step  0 the train_score is  0.6666667 the validation score is  0.683 this cost 6.392651081085205\n",
      "At step  1 the train_score is  0.875 the validation score is  0.8058 this cost 6.126388788223267\n",
      "At step  2 the train_score is  1.0 the validation score is  0.8464 this cost 6.125507831573486\n",
      "At step  3 the train_score is  1.0 the validation score is  0.8686 this cost 6.352351427078247\n",
      "At step  4 the train_score is  1.0 the validation score is  0.8794 this cost 6.359858989715576\n",
      "At step  5 the train_score is  1.0 the validation score is  0.8918 this cost 6.308007001876831\n",
      "At step  6 the train_score is  1.0 the validation score is  0.8988 this cost 6.431229829788208\n",
      "At step  7 the train_score is  1.0 the validation score is  0.9032 this cost 6.345709800720215\n",
      "At step  8 the train_score is  1.0 the validation score is  0.907 this cost 6.239832878112793\n",
      "At step  9 the train_score is  1.0 the validation score is  0.9108 this cost 6.468474388122559\n",
      "At step  10 the train_score is  1.0 the validation score is  0.9142 this cost 6.025668621063232\n",
      "At step  11 the train_score is  1.0 the validation score is  0.9172 this cost 6.219713926315308\n",
      "At step  12 the train_score is  1.0 the validation score is  0.9192 this cost 6.331557512283325\n",
      "At step  13 the train_score is  1.0 the validation score is  0.9206 this cost 6.003652095794678\n",
      "At step  14 the train_score is  1.0 the validation score is  0.9218 this cost 6.336045980453491\n",
      "At step  15 the train_score is  1.0 the validation score is  0.9234 this cost 6.326993465423584\n",
      "At step  16 the train_score is  1.0 the validation score is  0.926 this cost 6.485758543014526\n",
      "At step  17 the train_score is  1.0 the validation score is  0.9268 this cost 6.018610000610352\n",
      "At step  18 the train_score is  1.0 the validation score is  0.928 this cost 5.79888653755188\n",
      "At step  19 the train_score is  1.0 the validation score is  0.9302 this cost 5.819757699966431\n",
      "Run 20 steps total process time is 124.52114081382751\n",
      "the test set accuracy is 0.9211\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    total_start_time = time.time()\n",
    "    for step in range (n_steps):\n",
    "        start_time = time.time ()\n",
    "        for i_batch in range (n_batch):\n",
    "            start = i_batch * batch_size\n",
    "            if i_batch != n_batch - 1:\n",
    "                end = start + batch_size\n",
    "            else:\n",
    "                end = data_size\n",
    "            sess.run (training_op, feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        train_score = accuracy.eval (feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        loss_val, validation_score = sess.run([loss, accuracy], feed_dict = {X: X_validation, y:y_validation})\n",
    "        end_time = time.time ()\n",
    "        run_time = end_time - start_time\n",
    "        print ('At step ', step, 'the train_score is ', train_score, 'the validation score is ', validation_score, 'this cost', run_time)\n",
    "    total_end_time = time.time ()\n",
    "    print ('Run', n_steps,  'steps total process time is',  total_end_time - total_start_time)\n",
    "    accuracy_val = accuracy.eval (feed_dict = {X: X_test, y: y_test})\n",
    "    print ('the test set accuracy is', accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FtrlOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph ()\n",
    "\n",
    "import time\n",
    "N_feature = X_test.shape[1]\n",
    "n_layers = 5\n",
    "n_neurons = 100\n",
    "learning_rate = 0.01\n",
    "n_output = 10\n",
    "clip_threshold = 1\n",
    "momentum_value = 0.9\n",
    "\n",
    "X = tf.placeholder (tf.float32, shape = (None, N_feature), name = 'X')\n",
    "y = tf.placeholder (tf.int32, shape = (None), name = 'y')\n",
    "\n",
    "with tf.name_scope ('DNN_chapter11_8'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer (seed = 1)\n",
    "    inputs = X\n",
    "    for layer in range (n_layers):\n",
    "        inputs = tf.layers.dense (inputs, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name=\"hidden%d\" % (layer + 1))\n",
    "    y_predit = tf.layers.dense (inputs, n_output, kernel_initializer = he_init, name = 'output')\n",
    "    y_proba = tf.nn.softmax(y_predit, name=\"Y_proba\")\n",
    "\n",
    "with tf.name_scope ('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_predit)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.FtrlOptimizer(learning_rate = learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(y_predit, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "n_steps = 20\n",
    "batch_size = 32\n",
    "data_size = X_train.shape[0]\n",
    "n_batch = int (np.ceil (data_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step  0 the train_score is  0.125 the validation score is  0.1126 this cost 6.807048797607422\n",
      "At step  1 the train_score is  0.125 the validation score is  0.1126 this cost 6.715728282928467\n",
      "At step  2 the train_score is  0.125 the validation score is  0.1126 this cost 6.106866359710693\n",
      "At step  3 the train_score is  0.125 the validation score is  0.1126 this cost 5.94976806640625\n",
      "At step  4 the train_score is  0.125 the validation score is  0.1126 this cost 5.927315950393677\n",
      "At step  5 the train_score is  0.125 the validation score is  0.1126 this cost 5.909650802612305\n",
      "At step  6 the train_score is  0.125 the validation score is  0.1126 this cost 5.941744565963745\n",
      "At step  7 the train_score is  0.125 the validation score is  0.1126 this cost 5.891998529434204\n",
      "At step  8 the train_score is  0.125 the validation score is  0.1126 this cost 5.908383131027222\n",
      "At step  9 the train_score is  0.125 the validation score is  0.1126 this cost 5.899424314498901\n",
      "At step  10 the train_score is  0.125 the validation score is  0.1126 this cost 5.902414798736572\n",
      "At step  11 the train_score is  0.125 the validation score is  0.1126 this cost 5.898768663406372\n",
      "At step  12 the train_score is  0.125 the validation score is  0.1126 this cost 5.899437665939331\n",
      "At step  13 the train_score is  0.125 the validation score is  0.1126 this cost 6.2929933071136475\n",
      "At step  14 the train_score is  0.125 the validation score is  0.1126 this cost 5.915769815444946\n",
      "At step  15 the train_score is  0.125 the validation score is  0.1126 this cost 5.910628318786621\n",
      "At step  16 the train_score is  0.125 the validation score is  0.1126 this cost 5.940311908721924\n",
      "At step  17 the train_score is  0.125 the validation score is  0.1126 this cost 5.904378414154053\n",
      "At step  18 the train_score is  0.125 the validation score is  0.1126 this cost 5.905919551849365\n",
      "At step  19 the train_score is  0.125 the validation score is  0.1126 this cost 5.967155933380127\n",
      "Run 20 steps total process time is 120.59872913360596\n",
      "the test set accuracy is 0.1135\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    total_start_time = time.time()\n",
    "    for step in range (n_steps):\n",
    "        start_time = time.time ()\n",
    "        for i_batch in range (n_batch):\n",
    "            start = i_batch * batch_size\n",
    "            if i_batch != n_batch - 1:\n",
    "                end = start + batch_size\n",
    "            else:\n",
    "                end = data_size\n",
    "            sess.run (training_op, feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        train_score = accuracy.eval (feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        loss_val, validation_score = sess.run([loss, accuracy], feed_dict = {X: X_validation, y:y_validation})\n",
    "        end_time = time.time ()\n",
    "        run_time = end_time - start_time\n",
    "        print ('At step ', step, 'the train_score is ', train_score, 'the validation score is ', validation_score, 'this cost', run_time)\n",
    "    total_end_time = time.time ()\n",
    "    print ('Run', n_steps,  'steps total process time is',  total_end_time - total_start_time)\n",
    "    accuracy_val = accuracy.eval (feed_dict = {X: X_test, y: y_test})\n",
    "    print ('the test set accuracy is', accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProximalGradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step  0 the train_score is  1.0 the validation score is  0.9356 this cost 5.803193092346191\n",
      "At step  1 the train_score is  1.0 the validation score is  0.952 this cost 5.648366928100586\n",
      "At step  2 the train_score is  1.0 the validation score is  0.9586 this cost 5.634929180145264\n",
      "At step  3 the train_score is  1.0 the validation score is  0.9634 this cost 5.813776016235352\n",
      "At step  4 the train_score is  1.0 the validation score is  0.9674 this cost 5.643099784851074\n",
      "At step  5 the train_score is  1.0 the validation score is  0.969 this cost 5.659044027328491\n",
      "At step  6 the train_score is  1.0 the validation score is  0.9698 this cost 5.649252653121948\n",
      "At step  7 the train_score is  1.0 the validation score is  0.97 this cost 5.6877360343933105\n",
      "At step  8 the train_score is  1.0 the validation score is  0.9704 this cost 5.67298150062561\n",
      "At step  9 the train_score is  1.0 the validation score is  0.9706 this cost 5.6518800258636475\n",
      "At step  10 the train_score is  1.0 the validation score is  0.9712 this cost 5.6498918533325195\n",
      "At step  11 the train_score is  1.0 the validation score is  0.9714 this cost 5.649137496948242\n",
      "At step  12 the train_score is  1.0 the validation score is  0.9734 this cost 5.64905858039856\n",
      "At step  13 the train_score is  1.0 the validation score is  0.9742 this cost 5.697755575180054\n",
      "At step  14 the train_score is  1.0 the validation score is  0.9752 this cost 5.657988548278809\n",
      "At step  15 the train_score is  1.0 the validation score is  0.9748 this cost 5.644918203353882\n",
      "At step  16 the train_score is  1.0 the validation score is  0.9752 this cost 5.65837836265564\n",
      "At step  17 the train_score is  1.0 the validation score is  0.9752 this cost 5.639637470245361\n",
      "At step  18 the train_score is  1.0 the validation score is  0.9762 this cost 5.685257911682129\n",
      "At step  19 the train_score is  1.0 the validation score is  0.9758 this cost 5.64927339553833\n",
      "Run 20 steps total process time is 113.44756174087524\n",
      "the test set accuracy is 0.97\n"
     ]
    }
   ],
   "source": [
    "reset_graph ()\n",
    "\n",
    "import time\n",
    "N_feature = X_test.shape[1]\n",
    "n_layers = 5\n",
    "n_neurons = 100\n",
    "learning_rate = 0.01\n",
    "n_output = 10\n",
    "clip_threshold = 1\n",
    "momentum_value = 0.9\n",
    "\n",
    "X = tf.placeholder (tf.float32, shape = (None, N_feature), name = 'X')\n",
    "y = tf.placeholder (tf.int32, shape = (None), name = 'y')\n",
    "\n",
    "with tf.name_scope ('DNN_chapter11_8'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer (seed = 1)\n",
    "    inputs = X\n",
    "    for layer in range (n_layers):\n",
    "        inputs = tf.layers.dense (inputs, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name=\"hidden%d\" % (layer + 1))\n",
    "    y_predit = tf.layers.dense (inputs, n_output, kernel_initializer = he_init, name = 'output')\n",
    "    y_proba = tf.nn.softmax(y_predit, name=\"Y_proba\")\n",
    "\n",
    "with tf.name_scope ('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_predit)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.ProximalGradientDescentOptimizer(learning_rate = learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(y_predit, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "n_steps = 20\n",
    "batch_size = 32\n",
    "data_size = X_train.shape[0]\n",
    "n_batch = int (np.ceil (data_size / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    total_start_time = time.time()\n",
    "    for step in range (n_steps):\n",
    "        start_time = time.time ()\n",
    "        for i_batch in range (n_batch):\n",
    "            start = i_batch * batch_size\n",
    "            if i_batch != n_batch - 1:\n",
    "                end = start + batch_size\n",
    "            else:\n",
    "                end = data_size\n",
    "            sess.run (training_op, feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        train_score = accuracy.eval (feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        loss_val, validation_score = sess.run([loss, accuracy], feed_dict = {X: X_validation, y:y_validation})\n",
    "        end_time = time.time ()\n",
    "        run_time = end_time - start_time\n",
    "        print ('At step ', step, 'the train_score is ', train_score, 'the validation score is ', validation_score, 'this cost', run_time)\n",
    "    total_end_time = time.time ()\n",
    "    print ('Run', n_steps,  'steps total process time is',  total_end_time - total_start_time)\n",
    "    accuracy_val = accuracy.eval (feed_dict = {X: X_test, y: y_test})\n",
    "    print ('the test set accuracy is', accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProximalAdagradOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step  0 the train_score is  1.0 the validation score is  0.9526 this cost 5.705142021179199\n",
      "At step  1 the train_score is  1.0 the validation score is  0.9628 this cost 5.7057108879089355\n",
      "At step  2 the train_score is  1.0 the validation score is  0.968 this cost 5.620474815368652\n",
      "At step  3 the train_score is  1.0 the validation score is  0.9698 this cost 5.634216070175171\n",
      "At step  4 the train_score is  1.0 the validation score is  0.972 this cost 5.629890441894531\n",
      "At step  5 the train_score is  1.0 the validation score is  0.9734 this cost 5.632022857666016\n",
      "At step  6 the train_score is  1.0 the validation score is  0.9746 this cost 5.684013605117798\n",
      "At step  7 the train_score is  1.0 the validation score is  0.975 this cost 5.673597812652588\n",
      "At step  8 the train_score is  1.0 the validation score is  0.9756 this cost 5.906827449798584\n",
      "At step  9 the train_score is  1.0 the validation score is  0.9758 this cost 5.939324140548706\n",
      "At step  10 the train_score is  1.0 the validation score is  0.976 this cost 5.901564598083496\n",
      "At step  11 the train_score is  1.0 the validation score is  0.976 this cost 6.271184921264648\n",
      "At step  12 the train_score is  1.0 the validation score is  0.9768 this cost 5.635300397872925\n",
      "At step  13 the train_score is  1.0 the validation score is  0.9772 this cost 5.630412817001343\n",
      "At step  14 the train_score is  1.0 the validation score is  0.977 this cost 5.669353008270264\n",
      "At step  15 the train_score is  1.0 the validation score is  0.9766 this cost 5.633962631225586\n",
      "At step  16 the train_score is  1.0 the validation score is  0.977 this cost 5.632994174957275\n",
      "At step  17 the train_score is  1.0 the validation score is  0.977 this cost 5.63587760925293\n",
      "At step  18 the train_score is  1.0 the validation score is  0.977 this cost 5.656357765197754\n",
      "At step  19 the train_score is  1.0 the validation score is  0.9772 this cost 5.665917634963989\n",
      "Run 20 steps total process time is 114.46514558792114\n",
      "the test set accuracy is 0.9748\n"
     ]
    }
   ],
   "source": [
    "reset_graph ()\n",
    "\n",
    "import time\n",
    "N_feature = X_test.shape[1]\n",
    "n_layers = 5\n",
    "n_neurons = 100\n",
    "learning_rate = 0.01\n",
    "n_output = 10\n",
    "clip_threshold = 1\n",
    "momentum_value = 0.9\n",
    "\n",
    "X = tf.placeholder (tf.float32, shape = (None, N_feature), name = 'X')\n",
    "y = tf.placeholder (tf.int32, shape = (None), name = 'y')\n",
    "\n",
    "with tf.name_scope ('DNN_chapter11_8'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer (seed = 1)\n",
    "    inputs = X\n",
    "    for layer in range (n_layers):\n",
    "        inputs = tf.layers.dense (inputs, n_neurons, activation = tf.nn.elu, kernel_initializer = he_init, name=\"hidden%d\" % (layer + 1))\n",
    "    y_predit = tf.layers.dense (inputs, n_output, kernel_initializer = he_init, name = 'output')\n",
    "    y_proba = tf.nn.softmax(y_predit, name=\"Y_proba\")\n",
    "\n",
    "with tf.name_scope ('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_predit)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.ProximalAdagradOptimizer(learning_rate = learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(y_predit, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "n_steps = 20\n",
    "batch_size = 32\n",
    "data_size = X_train.shape[0]\n",
    "n_batch = int (np.ceil (data_size / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    total_start_time = time.time()\n",
    "    for step in range (n_steps):\n",
    "        start_time = time.time ()\n",
    "        for i_batch in range (n_batch):\n",
    "            start = i_batch * batch_size\n",
    "            if i_batch != n_batch - 1:\n",
    "                end = start + batch_size\n",
    "            else:\n",
    "                end = data_size\n",
    "            sess.run (training_op, feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        train_score = accuracy.eval (feed_dict = {X: X_train[start: end], y: y_train[start: end]})\n",
    "        loss_val, validation_score = sess.run([loss, accuracy], feed_dict = {X: X_validation, y:y_validation})\n",
    "        end_time = time.time ()\n",
    "        run_time = end_time - start_time\n",
    "        print ('At step ', step, 'the train_score is ', train_score, 'the validation score is ', validation_score, 'this cost', run_time)\n",
    "    total_end_time = time.time ()\n",
    "    print ('Run', n_steps,  'steps total process time is',  total_end_time - total_start_time)\n",
    "    accuracy_val = accuracy.eval (feed_dict = {X: X_test, y: y_test})\n",
    "    print ('the test set accuracy is', accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
