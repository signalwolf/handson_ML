# 机器学习的核心是：模型表征+模型评估+优化算法  
例如SVM = 线性分类模型 + 最大间隔；LR = 线性分类模型 + 交叉熵


## 目录
[Main Steps for machine learning](#Main-Steps-for-machine-learning)  
[Visualize the data](#Visualize-the-data)  
[Prepare the data for machine learning 数据处理](#Prepare-the-data-for-machine-learning-数据处理)  
[Select and train a Model](#Select-and-train-a-Model)  
[Fine Tune Your Model](#Fine-Tune-Your-Model)  
1.  [超参数调优](#超参数调优)    
2.  [过拟合与欠拟合](#过拟合与欠拟合)  

[Evaluate your model](#Evaluate-your-model) 
1.  [判断data的距离](#判断data的距离)
2.  [评估模型的方法](#评估模型的方法)
1.  [Performance Measurement for binary classification，ROC不是很懂，需要细看](#Performance-Measurement-for-binary-classification)  
2.  [Performance Measurement for linear regression](#Performance-Measurement-for-linear-regression) 
3.  [A/B 测试](#A/B-测试)

[Optimize your model](#Optimize-your-model(Supervised learning))


### Main Steps for machine learning:
1.  Look at the big picture
2.  Get the data
3.  Discover and visualize the data to gain insights
4.  Prepare the data for Machine learning algorithm
5.  Select a model and train it
6.  Fine-tune your model
7.  Present your solution
8.  Launch, monitor and maintain your system.

### Visualize the data
1.  housing.head(): get the top five rows's details
2.  housing.info(): get a quick description of the data, in particular the total number of rows
and each attribute's type and number of non-null values
3.  housing['a'].value_counts(): how many districts belongs to each category
4.  housing.describe(): a summary of the numerical attributes

### Prepare the data for machine learning 数据处理
1.  Create a Test set:

```
import numpy as np
def split_train_set(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) & test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]
```
```
# sklearn solution:
from sklearn.model_selection import train_test_split
train_set, test_set = train_test_split(housing, test_size = 0.2, random_state = 42)
```
2.  数据的处理
    1.  Deal with missing features: drop data one with missing feature; remove the feature completely;
    set the value to zero/mean/median
    ```
    housing.dropna(subset = ['total_bedrooms'])
    housing.drop('total_bedrooms', axis = 1)
    median = housing['total_bedrooms'].median()
    housing['total_bedrooms'].fillna(median, inplace = True)
    ```
    2.  Handling text and categorical attributes/feature: convert those to numbers
        1.  Most of machine learning algorithm needed to translate categorical feature to numbers. Only decision tree can accept string as an input.  
        2.  Type of convert:
            1.  One hot encoding (each entity is independent and no order):
                1.  For example: Blood Type A, B, AB, O:
                    1.  A --> [1, 0 ,0 ,0]
                    2.  B --> [0, 1, 0, 0]
                    3.  AB -> [0, 0, 1, 0]
                    4.  O --> [0, 0, 0, 1]
                2.  With one hot encoding, all input get an independent presentation. But cost too many space
                3.  使用稀疏向量能够节省空间，并且大多数的算法都接受稀疏向量作为input
                4.  或者使用降维的手段
            2.  Ordinal Encoding (each one related and have order, like score A is better than B, B is better than C)
            then 
                1.  A --> 3
                2.  B --> 2
                3.  C --> 1
            3.  Binary Encoding (使用二进制数来表示)：
                1.  A --> [0, 0, 0, 1]
                2.  B --> [0, 0, 1, 0]
                3. AB --> [0, 1, 1, 0]
                4.  O --> [1, 0, 0, 0]
 
    3.  Feature scaling/Normalization: help to get to the best answer faster. Almost all dx based machine learning algorithm
     need to use normalization. Others don't need, for example: decision tree/ naive bayes.
        1.  min_max scaling/ normalization; MinMaxScale
        2.  standardization scaling/Zero Score normalization (new data have zero mean and unit variance): StandardScaler
    
    4.  Transformation Pipelines: custom the process steps and for future use.
    
    5.  高维组合特征的处理：feature combine.
        1.  当feature的取值有恩多的时候，两个feature 的组合便有 m * n 个组合，如果使用on hot encoding的话那么就会造成严重的问题。
        所以需要降维来处理。例如购买物品是，用户ID 有千万个，物品ID也有千万个，而y就是用户是否点击，如果这样的话就造成了 m * n 的组合
        而这个组合的结果是非常大的。
        2.  故而我们将 feature combine, 我们的 X 中只记录： x1 = 用户ID = a 并点击了物品 ID == b 的情况。这样就删除了那些
        data，例如用户A 没有点击物品 B的情况
    
    6. 文本表示模型：
        1.  Bag of word: TF-IDF
            1.  TF-IDF(t,d) = TF(t,d) * IDF(t):
                1.  单词 t 在文档 d中出现的频率
                2.  单词 t 在所有的文档中出现的次数的反比 = log(文章总数/(包含单词t的文章总数 + 1))
        2.  N-gram: 考虑单词的前后连贯性，将 N 个单词作为一个 word t 来处理，这样也能得到一个结果。
        2.  Topic model: 找到具有代表性的文章然后做词频统计
        3.  World Embedding: Word2Vec: 最常用的词嵌入模型。
    
    7.  Word2Vec 是一种浅层神经网络，有两种网络结构：
        1.  CBOW: continuous bog of words: 通过上下文来预测当前词的生成概率。即输入是上下文，输出是当前词
        2.  Skip-gram: 根据当前的词来预测上下文中各词的生成概率。即输入是当前词，输出是当前词
    
    8.  图像处理中数据量不足的问题及解决方法：
        1.  问题：过拟合
        2.  解决方案：
            1.  在模型上：简化模型，Regularization, 集成学习，dropout
            2.  在数据上：Data Augmentation (图像的随机旋转，平移，缩放，裁剪)；对图像增加噪音扰动（高斯白噪声）
            颜色变换，改变亮度、清晰度、对比度等。
            3.  迁移学习 transfer learning: 用一个在大规模数据集上预训练好的通用模型做基础然后再针对现在的小的data进行微调。
   
        
### Select and train a Model:
Better Evaluation using cross-validation:
1.  Split the training set into a smaller training set and validation set.
2.  Train your model base on training set and evaluate against the validation set
3.  Easy way is to use sklearn's cross-validation feature.
        
        from sklearn.model_selection import cross_val_score
        scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring = 'neg_mean_squared_error', cv = 10)
        
### Fine Tune Your Model:
4.  #### 超参数调优：
    1.  grid search: 最简单但是应用最广
        1.  通过搜索超参数中所有的可能的组合来找出最优解。但是相应的极为消耗资源。
        2.  优化的方法是增加步长，这样搜索的组合数就会少很多，然后再在这些最优项中，减少步长再搜索，不断往复；这样消耗的资源会少很多，但是也可能会错过全局最小点
    2.  Random search: 
        1.  在搜索的上界和下届之间随机选取样本点，
        2.  优点是更快更少资源，但是也无法保证一定找到了全局最小点。
    3.  Bayers optimization:
        1.  上述二者的问题在于每次的选择都是独立的，故而上次的结果并没有在现在的情况中考虑。
        2.  但是贝叶斯方法比较容易停留在局部最小值上。
        3.  故而优化就是加入探索功能，每次会选择是探索还是利用。如果利用就使用之前信息更进一步，否则就在未取样点随机采样跳出之前的区域。
5.  #### 过拟合与欠拟合：
    1.  降低过拟合：Regularization, 增加data(data augmentation), Early drop, ensemble learning, reduce the complexity of the model(In NN, reduce the layers and nodes)
    2.  降低欠拟合: Add new features, increase the complexity of the model, reduce the regularization

### Evaluate your model
1.  #### 判断data的距离：
    1.  余弦距离/欧式距离
        1.  欧式距离代表了数值上的绝对差异，而余弦距离代表的是相对的差异。
        2.  例如说用户A的观看向量是（0，1）而B的观看向量为（1，0）。由于二者正交，故余弦相似度 = cos(90) = 0, 余弦距离 = 1 - 余弦相似度 = 1(max distance)；
        而欧式距离为 sqrt(2), 这样的欧式距离很小。故而可以说余弦距离代表着相对差异而欧式距离代表绝对差异。
        3.  不同的问题用不同的判断方法：例如对用户活跃度的分析，以登录时长与观看时间做feature的话，那么（1，10）与（10，100）的两个用户在余弦距离上相似，但是在
        欧式距离上很远。而此时应该用欧式距离。

2.  #### 评估模型的方法：
    1.  Holdout检验：
        1.  将原始数据随机分成 training set and validation set. Evaluate the result based on ROC curve, precision and recall...
        2.  缺点是在validation set上的performance与train/validation set分开有很大的影响。如果validation set不能反映真实分布，最好的结果也是最差的结果
    2.  Cross-validation:K fold: K == 10 most of time: 适用于data量比较大的情况
        1.  将数据分为K份，for i in xrange(K): validation_set = data[i], train_set = data[:i] + data[i + 1:].
        2.  即每次都将当前data当做validation set, 余下的所有都是training set
        3.  将K次的结果的平均值作为指标
    3.  Bootstrap:试用于data量比较小的情况
        1.  对于大小为 n 的数据集合，我们对它进行随机的n次的采样，得到大小为n的训练集。在n次采样中一直都没有被采样的样本被作为验证集进行模型验证。
        2.  在bootstrap中验证集有多少？ 对于一次采样来说：采到某点的概率为 1/n, 没有采到某点的概率为(1 - 1/n), 故而n次采样都没有采到某点的概率为 (1 - 1/n)**(n)
        3.  当n无穷大时，趋近于 1/e = 0.368 = 36.8%
    
1.  #### Performance Measurement for binary classification
    1.  T/F: True or False 代表的是在预测的情况。T:被分类器预测为正的个数，F:被分类器预测为负的个数
    2.  P/N: Positive or Negative: 代表的是真正的分布情况情况。T:真实的正样本的数量；N:真实的负样本的数量。
    例如医院检查是否患病，如果来人是病人的话，那就是 positive, 不是病了就是negative；医院可能检查出来了，然后检查出来有病的人
    只有一部分是真的有病，这部分人就是 True Positive, 相应的本来没病查出病了的就是 False Negative; 有病没查出来的就是
    False Positive; 没病也没查出病的就是 True Negative
    3.  Confusion Matrix: 
        1.  [True Negative, False Positive] 
        2.  [False Negative, True Positive]
    4.  Precision and recall: 
        1.  Precision: 精确率 TP/(TP + FP): 分类正确的正样本个数占分类器判定为正样本的样本个数的比例。
            1.  换而言之：你判断对的话，有多少是对的，即你的判断有多准确
            2.  故而分类器需要在非常有把握的时候才去做判断，也因此Precision的提高会造成Recall的降低。
        2.  Recall: 召回率 TP/(TP + PN)：分类正确的正样本个数占真正的正样本个数的比例
            1.  换而言之：应该是对的的你判断出了多少
    7.  准确率：正确的样本数占样本总数的比例。 --> 当正负样本比例相差较大的时候，准确率并无太大用处，例如99%的数据都是正向的，
    那么如果我的模型预估所有人都是正向的，那么准确率也是 99%。此时可以选择在每个category下的准确率的平均来评判。
    8.  PR曲线：横轴Precision, 纵轴Recall.
        1.  在下图中，当Recall很低时，modelA的performance肯定更好，相应的当recall很高时，modelB的performance更好。
    ![image](https://github.com/signalwolf/handson_ML/blob/master/Interview/Image/Screen%20Shot%202018-11-12%20at%204.09.08%20PM.png)
    6.  ROC Curve: used with binary classifiers. 这是binary classification的最重要的指标：
        1.  ROC的意思是：Receiver Operating Characteristic Curve.
        2.  横坐标：False Positive Rate: FPR = FP/N. 
        3.  纵坐标：True Positive Rate: TPR = TP/P
        4.  例子：假如有10个疑似患者，7个没有病，3个真有病；然后检查结果表示三个有病的人中只有两个是有病；在余下的7个没有病
        的人中发现有一个有病。故而：
            1.  T: 3; F: 7; P: 3; N: 7;
            2.  TP: 2; FP: 1; FN: 6; TN: 1;
            3.  TPR: TP/P = 2/3; FPR: FP/N = 1/7.
            4.  ROC图上的一点便由此画出：(2/3, 1/7)
            5.  ROC图上每个点所对应的是某个config下的FPR vs TPR, 例如说logsitic regression中我们设 0.5为 threshold,
            那么在0.5的config下会有一个ROC的点，同样，threshold == 0.1时也会产生另一个点。
        5.  使用ROC curve来判断性能：AUC: area under curve: 面积越大表示性能越好。其计算法非常简单，就是将曲线从0到1之间积分。得到的就是其面积
        AUC越大表明分类器越是可能将真正的样本排在前面，分类器性能就更好。
        6.  ROC curve与PR curve的对比：ROC curve有个明显的优势就是在正负样本比例增加后ROC curve不会改变但是PR曲线可能产生巨大的改变。如下图，这样的
        最大的好处就是能够消除不同的测试集所带来的干扰，更加客观的衡量模型的本身性能。被广泛的应用于排序、推荐、广告领域。当然如果更希望能够看到模型在特定的
        数据集上的表现，PR曲线更加直观
        ![image](https://github.com/signalwolf/handson_ML/blob/master/Interview/Image/Screen%20Shot%202018-11-12%20at%206.28.48%20PM.png)
    5.  F-score: F1 = 2 * (1/precision + 1/recall)，即精准率和召回率的调和平均值
    

2.  #### Performance Measurement for linear regression:
    1.  learning curve, plot 在training set上和validation set上的RMSE. 
    RMSE的公式：
    ![image](https://github.com/signalwolf/handson_ML/blob/master/Interview/Image/Screen%20Shot%202018-11-12%20at%204.13.54%20PM.png)
    2.  从上面的公式中我们能看到RMSE有个巨大的问题，那就是对outlier的处理，如果有个巨大的outlier，那么使用了
    RMSE的模型在迭代后会更加的靠近那个outlier，而使得99%的data的错误率都提高了。处理方法有：
        1.  认为这是噪声：清除掉
        2.  提高建模方式，将离群点产生机制也加入
        3.  找个更适合的指标来评估模型，如：MAPE: mean absolute present error:
        它由于对距离进行了归一化，使得outlier的影响变小了
    ![image](https://github.com/signalwolf/handson_ML/blob/master/Interview/Image/Screen%20Shot%202018-11-12%20at%204.22.05%20PM.png)
    3.  尽可能的使用多个指标对模型进行评估，这样能够看出当前的模型除了什么问题。例如MAPE很低而RMSE很高的话，就表明当前的模型对outlier的处理有严重的问题

3.  #### A/B 测试:
    1.  什么是A/B测试：在互联网公司中将用户分为A/B组然后A组比B组多一个feature，由此判断新算法新模型的效果。在机器学习领域，A/B测试是验证模型效果的最终手段。
    2.  为什么要做A/B测试：
        1.  因为offline model不能还原用户场景，例如说线上的时延、数据丢失、标签数据缺失等。因此离线模型是理想环境下的结果而非实际效果。
        2.  商业指标与模型指标可能不match，例如对模型的评判往往是ROC等，可是商业指标如点击率、存留时长等都无法得到。
    3.  如何划分对照组与实验组：
        1.  对target 范围内的用户随机生成 user ID --> 例如美国网站的target就是美国用户，故而其亚洲用户不需要参与。
        2.  user ID 为奇数的和偶数的分开为 A group 与 B group.
        3.  A group 用算法A，B group用算法B.

### Optimize your model (Supervised learning)
1.  Loss Function:
    1.  For classification:
        1.  0 - 1损失：L (f, y) = 1 if fy <=0. 
            1.  其实就是说如果预测的f与y的符号不一致就代表一个错误data，也可以认为是在0处划了一条线。
            2.  优点是计算起来非常简单
            3.  缺点非常不好，
                1.  其一：非凸函数，这样就很难找到全局最小点；
                2.  其二：不光滑，在小于0处，大于0处 gradiant == 0, 在等于0处，gradiant == 无穷
        2.  Hinge Function: L = max(0, 1 - fy): **SVM 的 loss function**
            1.  缺点是在0点出仍旧是不可导的
        3.  Logistic Function: L = log(1 + exp(-fy)): **logistic regression 的 loss function**
            1.  缺点是对所有的点都有惩罚，因此对异常值更加的敏感。
        4.  Cross Entropy Function:
        
        5.  Function 的实际情况：
    ![image](https://github.com/signalwolf/handson_ML/blob/master/Interview/Image/Screen%20Shot%202018-11-13%20at%2010.20.27%20PM.png)
    2.  For linear regression:
        1.  平方损失：Loss = (f - y) ** 2
            1.  当预测与差别较大的时候，惩罚力度非常大，造成它对异常点非常敏感
        2.  绝对损失：Loss=  |f -y|:
            1.  在 f == y 处无法求导是最大的问题
        3.  Huber Loss:
            1.  在 f 接近于 y的时候给一个值给他另一个函数。
