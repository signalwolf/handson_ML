#### Tensorflow:
```
import tensorflow as tf
with tf.Session() as sess:
```

### 神经网络训练时是否可以将全部参数初始化为0？
1.  不可以，因为没有gradient.甚至连取相同的值都不可以，因为这样无论正向传播还是反向传播都无法打破这种对称性，
试想每个参数都是一样的话，那么每次的gradient都是一样，于是造成了每次的update后的值都是一样的。最终导致
无法update，更何况如果是所有的参数初始都一样，那么公式便变为 y = a*x1 + a * x2 + a * x3 + a = 
a*(x1 + x2 + x3 + 1)
2.  初始的方法有很多种，一种就是 HE initialization.

### Dropout的工作原理：
1.  通过随机的抹除一个neuron的输出来使得整个系统不会非常的rely on一个neuron的输出。这样也使得Neuron之间的联合性被
打破了，同时也减少了过拟合的风险并由此增加了泛化的能力
2.  每次都随机的丢弃了一部分的Neuron所造成的后果就是每次training的神经网络都是不一样的。
3.  Dropout其实类似于bagging集成算法，是轻量级的集成算法。bagging算法同时traning了N个module,但是在神经网络的
训练中，如果要训练N个神经网络，那么开销肯定是巨大的。而dropout的丢弃也使得它成为一个轻量的集成算法。

### 批量归一化 BN (Batch Normalization)
1.  通过对每一层的输入都进行归一化处理，规避掉多层之后的数据在不同维度上有巨大的差异的情况。在巨大的差异时，gradient
的下降成为一个问题，会在不靠近中心的方向上进行较大的反复跳跃反而在向中心的方向上有较小的前进；同时差别较大的数据也会
造成网络的泛化能力大幅度的减小。


## CNN：核心思想是捕捉局部特征
### 卷积操作的本质特性包括稀疏交互，参数共享：
1.  稀疏交互(sparse interaction): 在普通的深层次神经网络中，每个neuron的input都连接着当层所有的neuron，故而如果有
M 个 input， N 个 neuron的话，连接的数量是 M*N个。对于高清的图片来说，这样的连接量太大了，故而CNN通过卷积使得只有一部
分的input影响当前的neuron，也就是输出神经元只与部分的前层的神经元连接了。这样便是spare 结构，它的复杂度是 K * N, 由于
K 远小于 M, 这样就造成了更迅速的算法，同时由于sparse连接，过拟合的情况得到了非常多的缓解。
    1.  稀疏交互的物理意义在于，图像、语音、文本的数据有局部结构特征，故而我们可以先学习局部的特征然后再在之上构建出更加
    复杂与抽象的特征。
2.  参数共享：如果我们把一个 (M, N )的图片当做是一个(M * N , 1)的数组的话，那么一个CNN的卷积核对图片的处理其实就是
这段内容进行了同样的参数的乘积。这样在做back propagation的时候update的参数数量得到了很大的减小。
    1.  在全连接网络中，每个参数只作用于一个neuron; 但是在卷积中，一个参数作用于每次卷积的局部输入的特定位置。故而这些个
    位置上都是shared
    
### 池化层：本质是降采样
1.  mean pooling: 对背景的保留效果更好
2.  max pooling:更好的提取纹理效果

### CNN用于语言的处理：
1.  